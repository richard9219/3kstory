# æœ¬åœ°åƒé—®æ¨¡å‹éƒ¨ç½²æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•åœ¨æœ¬åœ°éƒ¨ç½²å’Œä¼˜åŒ–é˜¿é‡Œå¼€æºçš„ Qwen æ¨¡å‹ç³»åˆ—,ç”¨äº 3kstory çŸ­å‰§ç”Ÿæˆå¹³å°ã€‚

---

## ğŸ“Œ å¿«é€Ÿå¯¹æ¯”

| æ–¹æ¡ˆ | vLLM | Ollama | LocalAI |
|------|------|--------|---------|
| **æ€§èƒ½** | â­â­â­â­â­ | â­â­â­ | â­â­â­ |
| **æ˜“ç”¨æ€§** | â­â­â­ | â­â­â­â­â­ | â­â­â­ |
| **æ˜¾å­˜å ç”¨** | 8GB | 6GB | 4GB |
| **ååé‡** | é«˜ | ä¸­ | ä½ |
| **æ¨èåœºæ™¯** | ç”Ÿäº§ç¯å¢ƒ | å¿«é€Ÿä½“éªŒ | ä½åŠŸè€— |

---

## ğŸš€ æ–¹æ¡ˆ 1ï¼švLLMï¼ˆæ¨èç”¨äºç”Ÿäº§ï¼‰

### ç‰¹ç‚¹
- æœ€é«˜çš„æ¨ç†é€Ÿåº¦ï¼ˆæ”¯æŒ PagedAttentionã€FlashAttentionï¼‰
- åŸç”Ÿæ”¯æŒ OpenAI API å…¼å®¹æ¥å£
- æ”¯æŒå¤šå¡å¹¶è¡Œã€å¼ é‡å¹¶è¡Œã€ç®¡é“å¹¶è¡Œ
- æ”¯æŒ LoRAã€é‡åŒ–ç­‰ä¼˜åŒ–

### ç¯å¢ƒè¦æ±‚

```bash
# ç¡¬ä»¶
GPU: NVIDIA A100/H100/RTX4090/RTX3090 (8GB+ VRAM)
CPU: 16+ æ ¸å¿ƒ
å†…å­˜: 32GB+

# è½¯ä»¶
CUDA 11.8+
Python 3.9+
```

### å®‰è£…æ­¥éª¤

#### 1. å®‰è£… vLLM

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate

# å®‰è£… vLLM
pip install vllm==0.3.0

# éªŒè¯å®‰è£…
python -c "from vllm import LLM; print('vLLM å®‰è£…æˆåŠŸ')"
```

#### 2. ä¸‹è½½ Qwen æ¨¡å‹

```bash
# ä½¿ç”¨ huggingface-cli ä¸‹è½½
pip install huggingface_hub

# ä¸‹è½½ Qwen2.5-7B-Instructï¼ˆæ¨èç”¨äºè„šæœ¬ç”Ÿæˆï¼‰
huggingface-cli download Qwen/Qwen2.5-7B-Instruct --local-dir ./models/qwen2.5-7b

# ä¸‹è½½ Qwen2-VLï¼ˆç”¨äºå†…å®¹å®¡æ ¸ï¼‰
huggingface-cli download Qwen/Qwen2-VL --local-dir ./models/qwen2-vl
```

#### 3. å¯åŠ¨ vLLM æœåŠ¡

```bash
# æ–¹å¼ Aï¼šç›´æ¥å¯åŠ¨ï¼ˆç”¨äºå¼€å‘ï¼‰
python -m vllm.entrypoints.openai.api_server \
  --model ./models/qwen2.5-7b \
  --gpu-memory-utilization 0.9 \
  --max-model-len 2048 \
  --host 0.0.0.0 \
  --port 8001

# æ–¹å¼ Bï¼šä½¿ç”¨ä¼˜åŒ–å‚æ•°å¯åŠ¨ï¼ˆç”¨äºç”Ÿäº§ï¼‰
python -m vllm.entrypoints.openai.api_server \
  --model ./models/qwen2.5-7b \
  --gpu-memory-utilization 0.95 \
  --max-model-len 4096 \
  --tensor-parallel-size 1 \
  --host 0.0.0.0 \
  --port 8001 \
  --trust-remote-code \
  --dtype auto \
  --enable-lora
```

#### 4. éªŒè¯æœåŠ¡

```bash
# æµ‹è¯• API
curl -X POST http://localhost:8001/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5-7b",
    "prompt": "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½",
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

### Docker éƒ¨ç½²ï¼ˆæ¨èï¼‰

åˆ›å»º `docker-compose-vllm.yml`ï¼š

```yaml
version: '3.8'

services:
  qwen-vllm:
    image: vllm/vllm-openai:latest
    container_name: qwen-vllm
    ports:
      - "8001:8000"
    volumes:
      - ./models/qwen2.5-7b:/workspace/qwen2.5-7b
      - ./models/qwen2-vl:/workspace/qwen2-vl
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - MODEL_NAME=qwen2.5-7b
    command: >
      python -m vllm.entrypoints.openai.api_server
        --model /workspace/qwen2.5-7b
        --gpu-memory-utilization 0.95
        --max-model-len 4096
        --host 0.0.0.0
        --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  default:
    name: 3kstory-network
```

å¯åŠ¨ï¼š

```bash
docker-compose -f docker-compose-vllm.yml up -d
```

---

## ğŸ’¾ æ–¹æ¡ˆ 2ï¼šOllamaï¼ˆå¿«é€Ÿä½“éªŒï¼‰

### ç‰¹ç‚¹
- æç®€éƒ¨ç½²ï¼ˆä¸€è¡Œå‘½ä»¤ï¼‰
- æ— éœ€ GPUï¼ˆæ”¯æŒ CPUï¼‰
- æ¨¡å‹ç®¡ç†ç®€å•
- ç¼ºç‚¹ï¼šæ€§èƒ½è¾ƒä½

### å®‰è£…æ­¥éª¤

#### 1. å®‰è£… Ollama

```bash
# macOS
brew install ollama

# Linux
curl https://ollama.ai/install.sh | sh

# Windows
# ä¸‹è½½ https://ollama.ai/download
```

#### 2. æ‹‰å– Qwen æ¨¡å‹

```bash
# å¯åŠ¨ Ollama å®ˆæŠ¤è¿›ç¨‹
ollama serve

# æ–°ç»ˆç«¯ï¼šä¸‹è½½æ¨¡å‹
ollama pull qwen:7b-chat

# åˆ—å‡ºå·²å®‰è£…æ¨¡å‹
ollama list
```

#### 3. æµ‹è¯• API

```bash
# é€šè¿‡ curl æµ‹è¯•
curl -X POST http://localhost:11434/api/generate \
  -d '{
    "model": "qwen:7b-chat",
    "prompt": "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œè¯·ç”¨ä¸­æ–‡ä»‹ç»ä¸€ä¸‹åŒ—äº¬",
    "stream": false
  }'
```

---

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–

### 1. é‡åŒ–ï¼ˆé™ä½æ˜¾å­˜å ç”¨ï¼‰

```bash
# ä½¿ç”¨ GPTQ 4-bit é‡åŒ–ç‰ˆæœ¬
huggingface-cli download Qwen/Qwen2.5-7B-Instruct-GPTQ \
  --local-dir ./models/qwen2.5-7b-gptq

# å¯åŠ¨ vLLMï¼ˆè‡ªåŠ¨è¯†åˆ«é‡åŒ–ï¼‰
python -m vllm.entrypoints.openai.api_server \
  --model ./models/qwen2.5-7b-gptq \
  --quantization gptq
```

**æ˜¾å­˜å¯¹æ¯”**ï¼š
- FP16 å®Œå…¨ç²¾åº¦ï¼š16GB
- INT8 é‡åŒ–ï¼š8GB
- INT4 é‡åŒ–ï¼ˆGPTQï¼‰ï¼š4GB

### 2. å¤šå¡æ¨ç†

```bash
# å¼ é‡å¹¶è¡Œï¼ˆå•æœºå¤šå¡ï¼‰
python -m vllm.entrypoints.openai.api_server \
  --model ./models/qwen2.5-7b \
  --tensor-parallel-size 2  # ä½¿ç”¨ 2 å¼  GPU
  --host 0.0.0.0 \
  --port 8001

# ç®¡é“å¹¶è¡Œï¼ˆå¤šå°æœºå™¨ï¼‰
python -m vllm.entrypoints.openai.api_server \
  --model ./models/qwen2.5-7b \
  --pipeline-parallel-size 2
```

### 3. æ‰¹å¤„ç†ä¼˜åŒ–

```python
# Go åç«¯è°ƒç”¨ç¤ºä¾‹
client := openai.NewClient("http://localhost:8001/v1")

// æ‰¹é‡è¯·æ±‚
requests := []struct {
    prompt string
    maxTokens int
}{
    {"åŒ—äº¬æ˜¯...", 100},
    {"ä¸Šæµ·æ˜¯...", 100},
}

// vLLM è‡ªåŠ¨æ‰¹å¤„ç†
for _, req := range requests {
    resp, _ := client.CreateCompletion(context.Background(), openai.CompletionRequest{
        Model: "qwen2.5-7b",
        Prompt: req.prompt,
        MaxTokens: req.maxTokens,
    })
}
```

### 4. LoRA å¾®è°ƒæ”¯æŒ

```bash
# ä½¿ç”¨ LoRA é€‚é…å™¨
python -m vllm.entrypoints.openai.api_server \
  --model ./models/qwen2.5-7b \
  --enable-lora \
  --lora-modules short-drama=/path/to/lora/weights
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

| æ¨¡å‹ | æ˜¾å­˜ | å»¶è¿Ÿ | ååé‡ |
|------|------|------|--------|
| Qwen2.5-7B-FP16 | 16GB | 100ms | 10 req/s |
| Qwen2.5-7B-INT8 | 8GB | 120ms | 8 req/s |
| Qwen2.5-7B-GPTQ | 4GB | 150ms | 6 req/s |
| Qwen2-VL | 12GB | 200ms | 5 req/s |

**æµ‹è¯•æ¡ä»¶**ï¼š
- è¾“å…¥é•¿åº¦ï¼š200 tokens
- è¾“å‡ºé•¿åº¦ï¼š100 tokens
- GPUï¼šRTX4090

---

## ğŸ”Œ ä¸åç«¯é›†æˆ

### Go ä»£ç ç¤ºä¾‹

```go
package services

import (
    "context"
    "github.com/sashabaranov/go-openai"
)

type LocalQwenConfig struct {
    BaseURL   string
    ModelName string
    MaxTokens int
}

type LocalQwenService struct {
    client *openai.Client
    config LocalQwenConfig
}

func NewLocalQwenService(cfg LocalQwenConfig) *LocalQwenService {
    client := openai.NewClientWithConfig(openai.DefaultConfig(
        "", // ä¸éœ€è¦ API Key
    ))
    client.BaseURL = cfg.BaseURL

    return &LocalQwenService{
        client: client,
        config: cfg,
    }
}

func (s *LocalQwenService) GenerateScript(ctx context.Context, prompt string) (string, error) {
    resp, err := s.client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
        Model: s.config.ModelName,
        Messages: []openai.ChatCompletionMessage{
            {
                Role:    openai.ChatMessageRoleUser,
                Content: prompt,
            },
        },
        MaxTokens:   s.config.MaxTokens,
        Temperature: 0.7,
    })
    
    if err != nil {
        return "", err
    }
    
    return resp.Choices[0].Message.Content, nil
}

func (s *LocalQwenService) ReviewContent(ctx context.Context, content string) (bool, error) {
    // ä½¿ç”¨ Qwen2-VL å®¡æ ¸å†…å®¹
    // ...
    return true, nil
}
```

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env æ–‡ä»¶
# é€‰æ‹© AI æä¾›å•†
AI_PROVIDER=local_qwen  # local_qwen | cloud_qwen | hybrid

# æœ¬åœ°æ¨¡å‹é…ç½®
LOCAL_QWEN_BASE_URL=http://localhost:8001/v1
LOCAL_QWEN_MODEL=qwen2.5-7b
LOCAL_QWEN_MAX_TOKENS=2048

# æˆ–ä½¿ç”¨ Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen:7b-chat
```

---

## ğŸ› ï¸ æ•…éšœæ’é™¤

### é—®é¢˜ 1ï¼šæ˜¾å­˜ä¸è¶³

```bash
# ç—‡çŠ¶
RuntimeError: CUDA out of memory

# è§£å†³æ–¹æ¡ˆ
# 1. ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬
quantization: gptq

# 2. é™ä½ max-model-len
--max-model-len 1024  # é»˜è®¤ 2048

# 3. ä½¿ç”¨å¼ é‡å¹¶è¡Œ
--tensor-parallel-size 2
```

### é—®é¢˜ 2ï¼šæ¨¡å‹åŠ è½½ç¼“æ…¢

```bash
# ç—‡çŠ¶
ç­‰å¾… 5+ åˆ†é’Ÿåæ‰èƒ½æ¥æ”¶è¯·æ±‚

# è§£å†³æ–¹æ¡ˆ
# 1. ä½¿ç”¨æœ¬åœ° SSD
# 2. å¢åŠ é¢„çƒ­æ—¶é—´

# 3. æ£€æŸ¥ç½‘ç»œï¼ˆé¦–æ¬¡ä¸‹è½½ï¼‰
huggingface-cli download Qwen/Qwen2.5-7B-Instruct --cache-dir /fast/ssd
```

### é—®é¢˜ 3ï¼šç²¾åº¦ä¸‹é™

```bash
# ç—‡çŠ¶
é‡åŒ–åè¾“å‡ºè´¨é‡æ˜æ˜¾ä¸‹é™

# è§£å†³æ–¹æ¡ˆ
# 1. ä½¿ç”¨ INT8ï¼ˆè€Œé INT4ï¼‰
# 2. è¿›è¡Œ LoRA å¾®è°ƒé€‚é…

# 3. è°ƒæ•´æ¸©åº¦å‚æ•°
temperature: 0.5  # æ›´ä¿å®ˆ
```

### é—®é¢˜ 4ï¼šAPI è¿”å› 404

```bash
# ç—‡çŠ¶
curl: (52) Empty reply from server

# è§£å†³æ–¹æ¡ˆ
# 1. æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨
ps aux | grep vllm

# 2. æ£€æŸ¥ç«¯å£æ˜¯å¦æ­£ç¡®
netstat -an | grep 8001

# 3. æŸ¥çœ‹æ—¥å¿—
docker logs qwen-vllm
```

---

## ğŸ“ˆ ç›‘æ§å’Œæ—¥å¿—

### vLLM æ—¥å¿—æ”¶é›†

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
python -m vllm.entrypoints.openai.api_server \
  --model ./models/qwen2.5-7b \
  --log-level debug

# æˆ–è®¾ç½®ç¯å¢ƒå˜é‡
export VLLM_LOGGING_LEVEL=debug
```

### æ€§èƒ½ç›‘æ§

```bash
# ç›‘æ§ GPU ä½¿ç”¨ç‡
nvidia-smi -l 1  # æ¯ç§’æ›´æ–°

# ç›‘æ§ API å“åº”æ—¶é—´
curl -w "@curl-format.txt" -o /dev/null -s http://localhost:8001/v1/completions
```

---

## ğŸ”„ æ›´æ–°å’Œå‡çº§

### æ›´æ–°æ¨¡å‹æƒé‡

```bash
# æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬
huggingface-cli download Qwen/Qwen2.5-7B-Instruct \
  --local-dir ./models/qwen2.5-7b \
  --force-download
```

### æ›´æ–° vLLM

```bash
# æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬
pip install --upgrade vllm

# æŸ¥çœ‹ç‰ˆæœ¬
python -c "import vllm; print(vllm.__version__)"
```

---

## ğŸ“š å‚è€ƒèµ„æº

- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [Qwen å®˜æ–¹æ¨¡å‹å¡](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
- [Ollama å®˜æ–¹æ–‡æ¡£](https://github.com/ollama/ollama)

---

**æ›´æ–°æ—¥æœŸ**ï¼š2024 å¹´ 12 æœˆ
